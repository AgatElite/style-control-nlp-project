{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Setup and data loading\n",
        "!pip install -q transformers torch scikit-learn matplotlib seaborn nltk accelerate bitsandbytes google-generativeai\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "from nltk.corpus import gutenberg\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.spatial.distance import cosine\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig, GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from huggingface_hub import snapshot_download\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on: {device}\")\n",
        "\n",
        "# Data Prep\n",
        "nltk.download('gutenberg', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def create_dataset(file_id, label, chunk_size=128, max_samples=300):\n",
        "    \"\"\"Tokenizes, chunks, and subsamples text from NLTK corpus.\"\"\"\n",
        "    raw = gutenberg.raw(file_id)\n",
        "    words = raw.split()\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        if len(chunk) > 200:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    if len(chunks) > max_samples:\n",
        "        random.seed(SEED)\n",
        "        chunks = random.sample(chunks, max_samples)\n",
        "\n",
        "    return pd.DataFrame({'text': chunks, 'label': label})\n",
        "\n",
        "# Load Corpus\n",
        "df = pd.concat([\n",
        "    create_dataset('austen-emma.txt', 'Austen'),\n",
        "    create_dataset('melville-moby_dick.txt', 'Melville')\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "print(f\"Dataset loaded: {len(df)} samples\")\n",
        "print(df['label'].value_counts())"
      ],
      "metadata": {
        "id": "ct_pg4xnPJdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BAAI/bge-small-en-v1.5 for semantic clustering\n",
        "SEM_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
        "\n",
        "print(f\"Loading {SEM_MODEL_NAME}...\")\n",
        "sem_tokenizer = AutoTokenizer.from_pretrained(SEM_MODEL_NAME)\n",
        "sem_model = AutoModel.from_pretrained(SEM_MODEL_NAME).to(device)\n",
        "\n",
        "def get_semantic_embeddings(text_list, batch_size=32):\n",
        "    sem_model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i+batch_size]\n",
        "        inputs = sem_tokenizer(batch, padding=True, truncation=True,\n",
        "                               max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = sem_model(**inputs)\n",
        "\n",
        "        # BGE uses CLS token normalized\n",
        "        cls_emb = out.last_hidden_state[:, 0]\n",
        "        cls_emb = torch.nn.functional.normalize(cls_emb, p=2, dim=1)\n",
        "        embeddings.append(cls_emb.cpu().numpy())\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "print(\"Generating semantic embeddings...\")\n",
        "vectors_sem = get_semantic_embeddings(df['text'].tolist())\n",
        "print(f\"Output shape: {vectors_sem.shape}\")"
      ],
      "metadata": {
        "id": "5CBUrd_5PNLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dimensionality Reduction\n",
        "pca = PCA(n_components=2, random_state=SEED)\n",
        "pca_res = pca.fit_transform(vectors_sem)\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=30, random_state=SEED, n_iter=1000)\n",
        "tsne_res = tsne.fit_transform(vectors_sem)\n",
        "\n",
        "# Clustering Metrics\n",
        "kmeans = KMeans(n_clusters=2, random_state=SEED, n_init=10)\n",
        "clusters = kmeans.fit_predict(vectors_sem)\n",
        "sil_score = silhouette_score(vectors_sem, df['label'])\n",
        "\n",
        "# Plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# PCA\n",
        "sns.scatterplot(ax=axes[0], x=pca_res[:,0], y=pca_res[:,1], hue=df['label'], alpha=0.7)\n",
        "axes[0].set_title('PCA: Austen vs Melville')\n",
        "\n",
        "# t-SNE\n",
        "sns.scatterplot(ax=axes[1], x=tsne_res[:,0], y=tsne_res[:,1], hue=df['label'], alpha=0.7)\n",
        "axes[1].set_title('t-SNE: Austen vs Melville')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"Silhouette Score: {sil_score:.4f}\")\n",
        "print(\"\\nConfusion Matrix (Cluster vs Label):\")\n",
        "print(pd.crosstab(df['label'], clusters))"
      ],
      "metadata": {
        "id": "Yek3ZJIlPPoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation model setup\n",
        "try:\n",
        "    api_key = userdata.get('GOOGLE_API_KEY')\n",
        "    genai.configure(api_key=api_key)\n",
        "except Exception as e:\n",
        "    print(\"Error: Please set GOOGLE_API_KEY in Colab Secrets.\")\n",
        "\n",
        "# Using Flash for efficiency\n",
        "gen_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "def generate_rewrite(text, target_author, style_example=None):\n",
        "    \"\"\"\n",
        "    Rewrites text using In-Context Learning (ICL).\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are an expert literary editor.\n",
        "    TASK: Rewrite the text below to strictly mimic the writing style, vocabulary, and syntax of {target_author}.\n",
        "    CONSTRAINTS: Keep original meaning/narrative. Do not add intro/outro text.\n",
        "    \"\"\"\n",
        "\n",
        "    if style_example:\n",
        "        prompt += f'\\nSTYLE REFERENCE:\\n\"{style_example}\"\\n'\n",
        "\n",
        "    prompt += f'\\nORIGINAL TEXT:\\n\"{text}\"\\n\\nREWRITTEN TEXT:'\n",
        "\n",
        "    try:\n",
        "        response = gen_model.generate_content(\n",
        "            prompt,\n",
        "            generation_config=genai.types.GenerationConfig(temperature=0.7)\n",
        "        )\n",
        "        return response.text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Generation Error: {str(e)}\""
      ],
      "metadata": {
        "id": "Lilhf1GCPSHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment execution\n",
        "\n",
        "# 1. Select Samples\n",
        "sample_idx = 5\n",
        "sample_austen = df[df['label'] == 'Austen'].iloc[sample_idx]['text']\n",
        "\n",
        "# Select a random demonstration for ICL (Pan et al., 2024)\n",
        "demo_idx = 10\n",
        "style_demo = df[df['label'] == 'Melville'].iloc[demo_idx]['text']\n",
        "\n",
        "print(f\"--- ORIGINAL (Austen) ---\\n{sample_austen[:200]}...\\n\")\n",
        "\n",
        "# 2. Generate Experimental Sample\n",
        "# Passing the demo helps the model disentangle style from content\n",
        "rewritten_melville = generate_rewrite(sample_austen, \"Herman Melville\", style_example=style_demo)\n",
        "print(f\"--- REWRITTEN (Target: Melville) ---\\n{rewritten_melville[:200]}...\\n\")\n",
        "\n",
        "# 3. Embed Experiment\n",
        "# Reuse the BGE model from Cell 2 (ensure Cell 2 has run)\n",
        "exp_vectors = get_semantic_embeddings([sample_austen, rewritten_melville])\n",
        "\n",
        "# 4. Centroid Analysis\n",
        "c_austen = vectors_sem[df['label'] == 'Austen'].mean(axis=0)\n",
        "c_melville = vectors_sem[df['label'] == 'Melville'].mean(axis=0)\n",
        "\n",
        "d_orig_austen = cosine(exp_vectors[0], c_austen)\n",
        "d_new_melville = cosine(exp_vectors[1], c_melville)\n",
        "d_new_austen = cosine(exp_vectors[1], c_austen)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(\"SEMANTIC SPACE (BGE) RESULTS:\")\n",
        "print(f\"Original -> Austen Centroid:   {d_orig_austen:.4f}\")\n",
        "print(f\"Rewritten -> Melville Centroid: {d_new_melville:.4f}\")\n",
        "print(f\"Rewritten -> Austen Centroid:   {d_new_austen:.4f}\")"
      ],
      "metadata": {
        "id": "9k_yn21fPTeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload BGE configuration to output hidden states\n",
        "# I'm using the same model weights, but need to access the level 2 ones\n",
        "from transformers import AutoConfig\n",
        "\n",
        "print(f\"Configuring {SEM_MODEL_NAME} for layer access...\")\n",
        "config = AutoConfig.from_pretrained(SEM_MODEL_NAME)\n",
        "config.output_hidden_states = True\n",
        "\n",
        "layer_model = AutoModel.from_pretrained(SEM_MODEL_NAME, config=config).to(device)\n",
        "layer_tokenizer = AutoTokenizer.from_pretrained(SEM_MODEL_NAME)\n",
        "\n",
        "def get_layer_embeddings(text_list, layer_idx=2, batch_size=32):\n",
        "    layer_model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i+batch_size]\n",
        "        inputs = layer_tokenizer(batch, padding=True, truncation=True,\n",
        "                                 max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = layer_model(**inputs)\n",
        "\n",
        "        # Extract specific hidden layer\n",
        "        # Layer 0 = Embeddings, Layer 1-12 = Transformer Blocks\n",
        "        hidden_state = out.hidden_states[layer_idx]\n",
        "\n",
        "        # Mean Pooling\n",
        "        mask = inputs['attention_mask'].unsqueeze(-1).expand(hidden_state.size()).float()\n",
        "        sum_emb = torch.sum(hidden_state * mask, 1)\n",
        "        sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
        "        mean_emb = sum_emb / sum_mask\n",
        "\n",
        "        # L2 Normalize\n",
        "        mean_emb = torch.nn.functional.normalize(mean_emb, p=2, dim=1)\n",
        "        embeddings.append(mean_emb.cpu().numpy())\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# 1. Generate corpus vectors for Layer 2\n",
        "print(\"Generating Layer 2 embeddings...\")\n",
        "vectors_layer2 = get_layer_embeddings(df['text'].tolist(), layer_idx=2)\n",
        "\n",
        "# 2. Calculate Layer 2 Centroids\n",
        "c_austen_l2 = vectors_layer2[df['label'] == 'Austen'].mean(axis=0)\n",
        "c_melville_l2 = vectors_layer2[df['label'] == 'Melville'].mean(axis=0)\n",
        "\n",
        "# 3. Run Experiment\n",
        "if 'sample_austen' in locals() and 'rewritten_melville' in locals():\n",
        "    exp_vecs_l2 = get_layer_embeddings([sample_austen, rewritten_melville], layer_idx=2)\n",
        "\n",
        "    d_orig = cosine(exp_vecs_l2[0], c_austen_l2)\n",
        "    d_new_mel = cosine(exp_vecs_l2[1], c_melville_l2)\n",
        "    d_new_aus = cosine(exp_vecs_l2[1], c_austen_l2)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(\"LAYER 2 RESULTS (Syntax/Structure Focus):\")\n",
        "    print(f\"Original -> Austen Centroid:   {d_orig:.4f}\")\n",
        "    print(f\"Rewritten -> Melville Centroid: {d_new_mel:.4f}\")\n",
        "    print(f\"Rewritten -> Austen Centroid:   {d_new_aus:.4f}\")\n",
        "\n",
        "    if d_new_mel < d_new_aus:\n",
        "        print(\"\\nResult: Success. Layer 2 vector shifted to Target Cluster.\")\n",
        "    else:\n",
        "        print(\"\\nResult: Failure. Layer 2 vector remained in Source Cluster.\")\n",
        "else:\n",
        "    print(\"Skipping experiment: input variables not found (Run Cell 5 first).\")"
      ],
      "metadata": {
        "id": "9U4aqVz8P8Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fluency Evaluation\n",
        "# Implements Perplexity (PPL) to evaluate whether\n",
        "# the style transfer results are fluent English.\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "\n",
        "print(\"Loading GPT-2 for Perplexity calculation...\")\n",
        "# Using standard GPT-2 as the 'judge' of fluency\n",
        "ppl_model_id = \"gpt2\"\n",
        "ppl_model = GPT2LMHeadModel.from_pretrained(ppl_model_id).to(device)\n",
        "ppl_tokenizer = GPT2TokenizerFast.from_pretrained(ppl_model_id)\n",
        "\n",
        "def calculate_perplexity(text):\n",
        "    \"\"\"\n",
        "    Calculates PPL (Perplexity).\n",
        "    Lower is better (10-100 is normal human text; >100 is incoherent).\n",
        "    \"\"\"\n",
        "    encodings = ppl_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    max_length = ppl_model.config.n_positions\n",
        "    stride = 512\n",
        "    seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "    nlls = []\n",
        "    prev_end_loc = 0\n",
        "    for begin_loc in range(0, seq_len, stride):\n",
        "        end_loc = min(begin_loc + max_length, seq_len)\n",
        "        trg_len = end_loc - prev_end_loc\n",
        "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "        target_ids = input_ids.clone()\n",
        "        target_ids[:, :-trg_len] = -100\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = ppl_model(input_ids, labels=target_ids)\n",
        "            # Neg Log Likelihood\n",
        "            nlls.append(outputs.loss * trg_len)\n",
        "\n",
        "        prev_end_loc = end_loc\n",
        "        if end_loc == seq_len:\n",
        "            break\n",
        "\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
        "    return ppl.item()\n",
        "\n",
        "# Evaluation of generated texts\n",
        "if 'sample_austen' in locals() and 'rewritten_melville' in locals():\n",
        "    ppl_orig = calculate_perplexity(sample_austen)\n",
        "    ppl_new = calculate_perplexity(rewritten_melville)\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(\"FLUENCY METRICS (PPL - Lower is better):\")\n",
        "    print(f\"Original Text PPL:  {ppl_orig:.2f}\")\n",
        "    print(f\"Rewritten Text PPL: {ppl_new:.2f}\")\n",
        "\n",
        "    if ppl_new < 100:\n",
        "        print(\"Result: The rewritten text is fluent (PPL < 100).\")\n",
        "    else:\n",
        "        print(\"Result: The rewritten text shows signs of incoherence.\")"
      ],
      "metadata": {
        "id": "vannQqBFjgsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the attention of Layer 2 vs Layer 12 to see if\n",
        "# they focus on different parts of the sentence (Syntax vs Content).\n",
        "\n",
        "# Reusing the BGE model from Cell 6\n",
        "from transformers import AutoConfig\n",
        "\n",
        "VIZ_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
        "print(f\"Loading {VIZ_MODEL_NAME} with attention outputs...\")\n",
        "\n",
        "viz_config = AutoConfig.from_pretrained(VIZ_MODEL_NAME)\n",
        "viz_config.output_attentions = True\n",
        "viz_model = AutoModel.from_pretrained(VIZ_MODEL_NAME, config=viz_config).to(device)\n",
        "viz_tokenizer = AutoTokenizer.from_pretrained(VIZ_MODEL_NAME)\n",
        "\n",
        "def plot_attention_heatmap(text, layer_num, title):\n",
        "    inputs = viz_tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    tokens = viz_tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = viz_model(**inputs)\n",
        "\n",
        "    # Get attentions: Tuple of (batch, heads, seq, seq)\n",
        "    # Selecting the specific layer\n",
        "    layer_attention = outputs.attentions[layer_num][0]\n",
        "\n",
        "    # Average across all heads to get a general \"focus map\"\n",
        "    avg_attention = torch.mean(layer_attention, dim=0).cpu().numpy()\n",
        "\n",
        "    # Filter tokens for visualization\n",
        "    clean_tokens = [t.replace('##', '') for t in tokens]\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(avg_attention, xticklabels=clean_tokens, yticklabels=clean_tokens, cmap=\"viridis\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "if 'rewritten_melville' in locals():\n",
        "    # Taking a short slice of the rewritten text for readability\n",
        "    short_text = \" \".join(rewritten_melville.split()[:20])\n",
        "\n",
        "    print(f\"Visualizing attention for: '{short_text}...'\")\n",
        "\n",
        "    # Plot Layer 2 (Syntax/Style focus - \"Success\" layer)\n",
        "    # Pan et al. suggest style is local/structural\n",
        "    plot_attention_heatmap(short_text, layer_num=1, title=\"Layer 2 Attention (Structural/Stylistic)\")\n",
        "\n",
        "    # Plot Final Layer (Semantic/Content focus - \"Failure\" layer)\n",
        "    plot_attention_heatmap(short_text, layer_num=11, title=\"Layer 12 Attention (Semantic/Content)\")"
      ],
      "metadata": {
        "id": "dRERt2q-jiWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Style classifier (Accuracy Metric)\n",
        "# Implements the \"ACC\" metric from Pan et al. (2024) using a lightweight\n",
        "# Logistic Regression on top of the BGE embeddings.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. Training the Classifier\n",
        "# Using the vectors already created in Cell 2/6\n",
        "print(\"Training Style Classifier...\")\n",
        "clf = LogisticRegression(random_state=SEED)\n",
        "clf.fit(vectors_sem, df['label'])\n",
        "\n",
        "# Checking the accuracy on the original data\n",
        "train_acc = clf.score(vectors_sem, df['label'])\n",
        "print(f\"Classifier Accuracy on Training Data: {train_acc*100:.2f}%\")\n",
        "\n",
        "# 2. Evaluating the Experiment\n",
        "if 'sample_austen' in locals() and 'rewritten_melville' in locals():\n",
        "    # Embed the samples (using the same BGE model)\n",
        "    exp_vecs = get_semantic_embeddings([sample_austen, rewritten_melville])\n",
        "\n",
        "    # Predicting probabilities\n",
        "    # Returns [Prob_Austen, Prob_Melville]\n",
        "    probs_orig = clf.predict_proba(exp_vecs[0].reshape(1, -1))[0]\n",
        "    probs_new = clf.predict_proba(exp_vecs[1].reshape(1, -1))[0]\n",
        "\n",
        "    # Get class labels order\n",
        "    classes = clf.classes_\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(\"STYLE PROBABILITY SCORES:\")\n",
        "    print(f\"Original Text:   {probs_orig[0]*100:.1f}% {classes[0]}, {probs_orig[1]*100:.1f}% {classes[1]}\")\n",
        "    print(f\"Rewritten Text:  {probs_new[0]*100:.1f}% {classes[0]}, {probs_new[1]*100:.1f}% {classes[1]}\")\n",
        "\n",
        "    # Determine \"Accuracy\" (Did it flip?)\n",
        "    target_author = \"Melville\"\n",
        "    target_idx = list(classes).index(\"Melville\")\n",
        "\n",
        "    if probs_new[target_idx] > 0.5:\n",
        "        print(f\"\\nRESULT: Success. Classified as {target_author}.\")\n",
        "    else:\n",
        "        print(f\"\\nRESULT: Failure. Still classified as {classes[1-target_idx]}.\")"
      ],
      "metadata": {
        "id": "svj7V53UskK0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}