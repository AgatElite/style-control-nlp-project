{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "BOOKS_TO_DOWNLOAD = {\n",
        "    'Fyodor Dostoevsky': [\n",
        "        {'title': 'Crime and Punishment', 'id': 2554},\n",
        "        {'title': 'The Brothers Karamazov', 'id': 28054},\n",
        "    ],\n",
        "    'Charles Dickens': [\n",
        "        {'title': 'A Tale of Two Cities', 'id': 98},\n",
        "        {'title': 'Great Expectations', 'id': 1400},\n",
        "        {'title': 'Oliver Twist', 'id': 730},\n",
        "    ]\n",
        "}\n",
        "\n",
        "PROCESSED_DATA_PATH = 'final_corpus.csv'\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def download_gutenberg_text(book_id):\n",
        "    \"\"\"Downloads the plain text version of a book from Project Gutenberg.\"\"\"\n",
        "    url_patterns = [\n",
        "        f'https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt',\n",
        "        f'https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt',\n",
        "        f'https://www.gutenberg.org/files/{book_id}/{book_id}.txt'\n",
        "    ]\n",
        "    for url in url_patterns:\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                print(f\"  - Successfully downloaded from {url}\")\n",
        "                return response.content.decode('utf-8', errors='ignore')\n",
        "        except requests.exceptions.RequestException:\n",
        "            continue\n",
        "    print(f\"  - ERROR: Failed to download book with ID {book_id}.\")\n",
        "    return None\n",
        "\n",
        "def remove_table_of_contents(text):\n",
        "    \"\"\"\n",
        "    A new function to specifically find and remove lines that look like a Table of Contents.\n",
        "    \"\"\"\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "    in_toc_section = False\n",
        "\n",
        "    # A regex to identify typical TOC lines (e.g., \"Chapter I...\", \"Part 1...\")\n",
        "    # It looks for the keyword, a number/roman numeral, and not much else on the line.\n",
        "    toc_pattern = re.compile(r'^\\s*(chapter|part|book|stave|epilogue)\\s+([ivx\\d]+|[a-zA-Z]+)?\\s*(\\.|\\s)*\\s*$', re.IGNORECASE)\n",
        "\n",
        "    # Another pattern for lines ending in page numbers or many dots\n",
        "    toc_pattern_2 = re.compile(r'.*\\s\\.{3,}\\s*\\d*\\s*$|.*\\s_+\\s*\\d*\\s*$')\n",
        "\n",
        "    for line in lines:\n",
        "        # If a line matches a TOC pattern, we skip it.\n",
        "        if toc_pattern.match(line.strip()) or toc_pattern_2.match(line.strip()):\n",
        "            continue\n",
        "        # A simple heuristic: if a line is just \"CONTENTS\" or \"TABLE OF CONTENTS\", skip it\n",
        "        if line.strip().lower() in [\"contents\", \"table of contents\"]:\n",
        "            continue\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "\n",
        "\n",
        "def clean_gutenberg_text(text, book_title):\n",
        "    \"\"\"\n",
        "    The main cleaning pipeline, now including the TOC removal.\n",
        "    \"\"\"\n",
        "    # 1. Remove standard Gutenberg footer\n",
        "    end_marker = re.search(r'\\*\\*\\* END OF (THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*', text, re.IGNORECASE)\n",
        "    if end_marker: text = text[:end_marker.start()]\n",
        "\n",
        "    # 2. Remove standard Gutenberg header\n",
        "    start_marker = re.search(r'\\*\\*\\* START OF (THIS|THE) PROJECT GUTENBERG EBOOK .* \\*\\*\\*', text, re.IGNORECASE)\n",
        "    if start_marker: text = text[start_marker.end():]\n",
        "\n",
        "    # 3. *** NEW STEP *** Remove the Table of Contents\n",
        "    text = remove_table_of_contents(text)\n",
        "\n",
        "    # 4. Final whitespace cleanup\n",
        "    text = re.sub(r'(\\r\\n|\\n|\\r){3,}', '\\n\\n', text).strip()\n",
        "    return text\n",
        "\n",
        "def segment_text(text, min_chunk_length=50):\n",
        "    \"\"\"Splits a long text into smaller chunks (paragraphs).\"\"\"\n",
        "    if not isinstance(text, str): return []\n",
        "    chunks = re.split(r'\\n{2,}', text)\n",
        "    return [chunk.strip() for chunk in chunks if len(chunk.strip()) >= min_chunk_length]\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Performs basic text normalization.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Main Processing Pipeline ---\n",
        "\n",
        "def main():\n",
        "    processed_data = []\n",
        "    print(\"Starting FINAL data acquisition and cleaning pipeline...\")\n",
        "\n",
        "    for author, books in BOOKS_TO_DOWNLOAD.items():\n",
        "        print(f\"\\nProcessing books for: {author}\")\n",
        "        for book in books:\n",
        "            title, book_id = book['title'], book['id']\n",
        "            print(f\"- Downloading '{title}' (ID: {book_id})...\")\n",
        "\n",
        "            raw_text = download_gutenberg_text(book_id)\n",
        "            if not raw_text: continue\n",
        "\n",
        "            cleaned_text = clean_gutenberg_text(raw_text, title)\n",
        "            chunks = segment_text(cleaned_text)\n",
        "\n",
        "            for chunk in chunks:\n",
        "                normalized_chunk = normalize_text(chunk)\n",
        "                processed_data.append({\n",
        "                    'author': author,\n",
        "                    'book_title': title,\n",
        "                    'text_chunk': normalized_chunk\n",
        "                })\n",
        "            print(f\"  - Finished processing '{title}', created {len(chunks)} text chunks.\")\n",
        "\n",
        "    if not processed_data:\n",
        "        print(\"\\nPipeline finished, but no data was processed.\")\n",
        "        return\n",
        "\n",
        "    processed_df = pd.DataFrame(processed_data)\n",
        "    processed_df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
        "\n",
        "    print(\"\\n--------------------\")\n",
        "    print(\"Pipeline Complete!\")\n",
        "    print(f\"Total processed text chunks: {len(processed_df)}\")\n",
        "    print(f\"Final, cleaned data has been saved to '{PROCESSED_DATA_PATH}'\")\n",
        "    print(\"--------------------\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FINAL data acquisition and cleaning pipeline...\n",
            "\n",
            "Processing books for: Fyodor Dostoevsky\n",
            "- Downloading 'Crime and Punishment' (ID: 2554)...\n",
            "  - Successfully downloaded from https://www.gutenberg.org/files/2554/2554-0.txt\n",
            "  - Finished processing 'Crime and Punishment', created 3127 text chunks.\n",
            "- Downloading 'The Brothers Karamazov' (ID: 28054)...\n",
            "  - Successfully downloaded from https://www.gutenberg.org/files/28054/28054-0.txt\n",
            "  - Finished processing 'The Brothers Karamazov', created 4895 text chunks.\n",
            "\n",
            "Processing books for: Charles Dickens\n",
            "- Downloading 'A Tale of Two Cities' (ID: 98)...\n",
            "  - Successfully downloaded from https://www.gutenberg.org/files/98/98-0.txt\n",
            "  - Finished processing 'A Tale of Two Cities', created 2575 text chunks.\n",
            "- Downloading 'Great Expectations' (ID: 1400)...\n",
            "  - Successfully downloaded from https://www.gutenberg.org/files/1400/1400-0.txt\n",
            "  - Finished processing 'Great Expectations', created 3107 text chunks.\n",
            "- Downloading 'Oliver Twist' (ID: 730)...\n",
            "  - Successfully downloaded from https://www.gutenberg.org/files/730/730-0.txt\n",
            "  - Finished processing 'Oliver Twist', created 3396 text chunks.\n",
            "\n",
            "--------------------\n",
            "Pipeline Complete!\n",
            "Total processed text chunks: 17100\n",
            "Final, cleaned data has been saved to 'final_corpus.csv'\n",
            "--------------------\n",
            "\n",
            "Verifying the start of a previously problematic book (Great Expectations):\n",
            "my fatherâ€™s family name being pirrip, and my christian name philip, my infant tongue could make of both names nothing longer or more explicit than pip. so, i called myself pip, and came to be called p...\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "9Xp7al6BLt4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afef3b6e-00fd-48bd-f7e9-0affcda35ace"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPi7fdLrZAta"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}