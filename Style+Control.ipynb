{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import subprocess\n",
        "import zipfile\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# 1. KAGGLE DATASET IDENTIFIER\n",
        "# This is the unique ID for the dataset on Kaggle.\n",
        "KAGGLE_DATASET_ID = 'thedevastator/comprehensive-literary-greats-dataset'\n",
        "\n",
        "# The specific file we need from the downloaded zip archive.\n",
        "DATASET_FILENAME = 'books_1.Best_Books_Ever.csv'\n",
        "\n",
        "# 2. DEFINE AUTHORS FOR THE CORPUS\n",
        "AUTHORS = ['Fyodor Dostoevsky', 'Charles Dickens']\n",
        "\n",
        "# 3. CHOOSE PRE-TRAINED MODEL FOR TOKENIZATION\n",
        "# Your project ladder suggests roberta-base, which is an excellent choice.\n",
        "MODEL_NAME = 'roberta-base'\n",
        "\n",
        "# 4. OUTPUT FILE\n",
        "# This is where we'll save the clean, processed data.\n",
        "PROCESSED_DATA_PATH = 'processed_literary_corpus.csv'\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def download_dataset_from_kaggle(dataset_id, destination_file):\n",
        "    \"\"\"\n",
        "    Downloads and unzips a dataset from Kaggle using the Kaggle API.\n",
        "    This function is designed for a Google Colab environment.\n",
        "    \"\"\"\n",
        "    # If the file already exists, we don't need to download it again.\n",
        "    if os.path.exists(destination_file):\n",
        "        print(f\"Dataset '{destination_file}' already exists. Skipping download.\")\n",
        "        return True\n",
        "\n",
        "    print(\"--- Setting up Kaggle API for dataset download ---\")\n",
        "    # In Colab, the user needs to upload their kaggle.json file.\n",
        "    if not os.path.exists('kaggle.json'):\n",
        "        print(\"\\n[ACTION REQUIRED]\")\n",
        "        print(\"ERROR: 'kaggle.json' not found in the current Colab session.\")\n",
        "        print(\"To proceed, please do the following:\")\n",
        "        print(\"1. Go to your Kaggle account page, navigate to the 'API' section.\")\n",
        "        print(\"2. Click on 'Create New API Token'. This will download a 'kaggle.json' file.\")\n",
        "        print(\"3. In Google Colab, click the 'Files' icon on the left sidebar and upload the 'kaggle.json' file you just downloaded.\")\n",
        "        print(\"4. Rerun this cell after uploading.\")\n",
        "        return False\n",
        "\n",
        "    # Create the necessary directory and move the API key into it.\n",
        "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "    os.rename('kaggle.json', '/root/.kaggle/kaggle.json')\n",
        "    # Set the correct permissions for the API key file.\n",
        "    os.chmod('/root/.kaggle/kaggle.json', 600)\n",
        "\n",
        "    print(f\"Downloading dataset: {dataset_id}...\")\n",
        "    # The name of the downloaded zip file is typically the dataset name part of the ID.\n",
        "    zip_file_name = f\"{dataset_id.split('/')[1]}.zip\"\n",
        "\n",
        "    # Execute the Kaggle API command to download the dataset.\n",
        "    command = ['kaggle', 'datasets', 'download', '-d', dataset_id]\n",
        "    result = subprocess.run(command, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(\"\\nERROR: Kaggle download failed.\")\n",
        "        print(\"Please ensure your 'kaggle.json' is valid and you have accepted the dataset's terms on the Kaggle website.\")\n",
        "        print(\"Stderr:\", result.stderr)\n",
        "        return False\n",
        "\n",
        "    print(\"Download successful. Unzipping...\")\n",
        "    # Unzip the downloaded archive to extract our target file.\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "            if destination_file in zip_ref.namelist():\n",
        "                zip_ref.extract(destination_file, '.')\n",
        "                print(f\"Successfully extracted '{destination_file}'\")\n",
        "            else:\n",
        "                print(f\"ERROR: The expected file '{destination_file}' was not in the archive.\")\n",
        "                print(f\"Files found in zip: {zip_ref.namelist()}\")\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during unzipping: {e}\")\n",
        "        return False\n",
        "    finally:\n",
        "        # Clean up by removing the downloaded zip file.\n",
        "        if os.path.exists(zip_file_name):\n",
        "            os.remove(zip_file_name)\n",
        "\n",
        "    return True\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Loads the dataset from the specified CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"\\nAttempting to load data from: {path}\")\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the data: {e}\")\n",
        "        return None\n",
        "\n",
        "def segment_text(text, min_chunk_length=50):\n",
        "    \"\"\"\n",
        "    Splits a long text into smaller chunks (paragraphs).\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    chunks = re.split(r'\\n+', text)\n",
        "    return [chunk.strip() for chunk in chunks if len(chunk.strip()) >= min_chunk_length]\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Performs basic text normalization.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Main Processing Pipeline ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the data acquisition and preprocessing pipeline.\n",
        "    \"\"\"\n",
        "    # 1. Download the dataset from Kaggle if it doesn't exist locally.\n",
        "    if not download_dataset_from_kaggle(KAGGLE_DATASET_ID, DATASET_FILENAME):\n",
        "        print(\"\\nPipeline stopped: Could not acquire dataset.\")\n",
        "        return\n",
        "\n",
        "    # 2. Load the dataset\n",
        "    df = load_data(DATASET_FILENAME)\n",
        "    if df is None:\n",
        "        return\n",
        "\n",
        "    print(\"Dataset loaded successfully. Columns:\", df.columns.tolist())\n",
        "\n",
        "    # 3. Filter for the chosen authors\n",
        "    author_column = 'author'\n",
        "    if author_column not in df.columns:\n",
        "        print(f\"ERROR: Column '{author_column}' not found. Please check the dataset.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFiltering for authors: {AUTHORS}\")\n",
        "    corpus_df = df[df[author_column].isin(AUTHORS)].copy()\n",
        "    print(f\"Found {len(corpus_df)} books by the selected authors.\")\n",
        "\n",
        "    if corpus_df.empty:\n",
        "        print(\"No books by the specified authors were found.\")\n",
        "        return\n",
        "\n",
        "    # 4. Process each book\n",
        "    processed_data = []\n",
        "    text_column = 'text'\n",
        "    if text_column not in corpus_df.columns:\n",
        "        print(f\"ERROR: Column '{text_column}' not found. Please check the dataset.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nStarting text segmentation and normalization...\")\n",
        "    for index, row in corpus_df.iterrows():\n",
        "        book_title = row.get('book_title', 'Unknown Title')\n",
        "        author = row[author_column]\n",
        "        book_text = row[text_column]\n",
        "\n",
        "        chunks = segment_text(book_text)\n",
        "        for chunk in chunks:\n",
        "            normalized_chunk = normalize_text(chunk)\n",
        "            processed_data.append({\n",
        "                'author': author,\n",
        "                'book_title': book_title,\n",
        "                'text_chunk': normalized_chunk\n",
        "            })\n",
        "        print(f\"  - Processed '{book_title}' by {author}, created {len(chunks)} chunks.\")\n",
        "\n",
        "    # 5. Create and save the final processed DataFrame\n",
        "    processed_df = pd.DataFrame(processed_data)\n",
        "    processed_df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
        "    print(f\"\\nTotal processed text chunks: {len(processed_df)}\")\n",
        "    print(f\"Processed data has been saved to '{PROCESSED_DATA_PATH}'\")\n",
        "\n",
        "    # 6. Demonstrate tokenization on a sample\n",
        "    print(\"\\n--- Tokenization Example ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        sample_text = processed_df['text_chunk'].iloc[0]\n",
        "        print(f\"Sample Text:\\n'{sample_text}'\")\n",
        "        tokens = tokenizer.tokenize(sample_text)\n",
        "        print(f\"\\nTokens from '{MODEL_NAME}':\\n{tokens}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not demonstrate tokenization. Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Setting up Kaggle API for dataset download ---\n",
            "Downloading dataset: thedevastator/comprehensive-literary-greats-dataset...\n",
            "Download successful. Unzipping...\n",
            "ERROR: The expected file 'all_books_with_summaries.csv' was not in the archive.\n",
            "Files found in zip: ['books_1.Best_Books_Ever.csv']\n",
            "\n",
            "Pipeline stopped: Could not acquire dataset.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "9Xp7al6BLt4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532976eb-c1bf-4b1a-c70b-a97ea7c4de0f"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}