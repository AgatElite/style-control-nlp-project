{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# 1. DEFINE FILE PATHS\n",
        "# TODO: Update this to the actual path where you've downloaded the Kaggle dataset.\n",
        "# The \"Comprehensive Literary Greats Dataset\" from Kaggle is a great choice as suggested.\n",
        "# We'll assume the main file is named 'all_books_with_summaries.csv'.\n",
        "# If you use a different dataset, you may need to adjust the column names below.\n",
        "DATASET_PATH = 'path/to/your/all_books_with_summaries.csv'\n",
        "\n",
        "# 2. DEFINE AUTHORS FOR THE CORPUS\n",
        "AUTHORS = ['Fyodor Dostoevsky', 'Charles Dickens']\n",
        "\n",
        "# 3. CHOOSE PRE-TRAINED MODEL FOR TOKENIZATION\n",
        "# Your project ladder suggests roberta-base, which is an excellent choice.\n",
        "# We will use its tokenizer to prepare the text.\n",
        "MODEL_NAME = 'roberta-base'\n",
        "\n",
        "# 4. OUTPUT FILE\n",
        "# This is where we'll save the clean, processed data.\n",
        "PROCESSED_DATA_PATH = 'processed_literary_corpus.csv'\n",
        "\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "def load_data(path):\n",
        "    \"\"\"\n",
        "    Loads the dataset from the specified CSV file.\n",
        "    Handles potential FileNotFoundError.\n",
        "    \"\"\"\n",
        "    print(f\"Attempting to load data from: {path}\")\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"---\")\n",
        "        print(f\"ERROR: File not found at '{path}'\")\n",
        "        print(\"Please download the 'Comprehensive Literary Greats Dataset' from Kaggle\")\n",
        "        print(\"or another suitable dataset and update the DATASET_PATH variable in this script.\")\n",
        "        print(f\"---\")\n",
        "        return None\n",
        "    try:\n",
        "        # The dataset might be large, so let's be mindful of data types.\n",
        "        return pd.read_csv(path)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading the data: {e}\")\n",
        "        return None\n",
        "\n",
        "def segment_text(text, min_chunk_length=50):\n",
        "    \"\"\"\n",
        "    Splits a long text (like a full book) into smaller chunks.\n",
        "    A good heuristic is to split by double newlines, which often separate paragraphs.\n",
        "\n",
        "    Args:\n",
        "        text (str): The full text of the book.\n",
        "        min_chunk_length (int): The minimum character length for a chunk to be kept.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of text chunks (paragraphs).\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    # Split by one or more newline characters\n",
        "    chunks = re.split(r'\\n+', text)\n",
        "\n",
        "    # Filter out very short or empty chunks and strip leading/trailing whitespace\n",
        "    return [chunk.strip() for chunk in chunks if len(chunk.strip()) >= min_chunk_length]\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Performs basic text normalization:\n",
        "    1. Converts to lowercase.\n",
        "    2. Removes excessive whitespace.\n",
        "\n",
        "    Note: We are not removing punctuation here because it can be a stylistic feature.\n",
        "    The transformer's tokenizer is designed to handle punctuation correctly.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# --- Main Processing Pipeline ---\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the data acquisition and preprocessing pipeline.\n",
        "    \"\"\"\n",
        "    # 1. Load the dataset\n",
        "    df = load_data(DATASET_PATH)\n",
        "    if df is None:\n",
        "        return # Stop execution if data loading failed\n",
        "\n",
        "    print(\"Dataset loaded successfully. Columns:\", df.columns.tolist())\n",
        "\n",
        "    # 2. Filter for the chosen authors\n",
        "    # We'll assume the author information is in a column named 'author'.\n",
        "    # Adjust 'author' to the correct column name if it's different in your file.\n",
        "    author_column = 'author'\n",
        "    if author_column not in df.columns:\n",
        "        print(f\"ERROR: Column '{author_column}' not found in the dataset.\")\n",
        "        print(\"Please check the CSV file and update the 'author_column' variable.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nFiltering for authors: {AUTHORS}\")\n",
        "    corpus_df = df[df[author_column].isin(AUTHORS)].copy()\n",
        "    print(f\"Found {len(corpus_df)} books by the selected authors.\")\n",
        "\n",
        "    if corpus_df.empty:\n",
        "        print(\"No books by the specified authors were found. Please check the author names and the dataset.\")\n",
        "        return\n",
        "\n",
        "    # 3. Process each book\n",
        "    processed_data = []\n",
        "    text_column = 'text' # Assuming the book content is in a column named 'text'\n",
        "    if text_column not in corpus_df.columns:\n",
        "        print(f\"ERROR: Column '{text_column}' not found in the dataset.\")\n",
        "        print(\"Please check the CSV file and update the 'text_column' variable.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nStarting text segmentation and normalization for each book...\")\n",
        "    for index, row in corpus_df.iterrows():\n",
        "        book_title = row.get('book_title', 'Unknown Title')\n",
        "        author = row[author_column]\n",
        "        book_text = row[text_column]\n",
        "\n",
        "        # Segment the book's text into paragraphs/chunks\n",
        "        chunks = segment_text(book_text)\n",
        "\n",
        "        # Normalize each chunk and add it to our processed list\n",
        "        for chunk in chunks:\n",
        "            normalized_chunk = normalize_text(chunk)\n",
        "            processed_data.append({\n",
        "                'author': author,\n",
        "                'book_title': book_title,\n",
        "                'text_chunk': normalized_chunk\n",
        "            })\n",
        "        print(f\"  - Processed '{book_title}' by {author}, created {len(chunks)} chunks.\")\n",
        "\n",
        "    # 4. Create a new DataFrame with the processed data\n",
        "    processed_df = pd.DataFrame(processed_data)\n",
        "    print(f\"\\nTotal processed text chunks: {len(processed_df)}\")\n",
        "\n",
        "    # 5. Save the processed data to a new CSV file\n",
        "    processed_df.to_csv(PROCESSED_DATA_PATH, index=False)\n",
        "    print(f\"\\nProcessed data has been saved to '{PROCESSED_DATA_PATH}'\")\n",
        "\n",
        "    # 6. Demonstrate tokenization on a sample\n",
        "    print(\"\\n--- Tokenization Example ---\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "        sample_text = processed_df['text_chunk'].iloc[0]\n",
        "        print(f\"Sample Text:\\n'{sample_text}'\")\n",
        "\n",
        "        tokens = tokenizer.tokenize(sample_text)\n",
        "        print(f\"\\nTokens produced by '{MODEL_NAME}' tokenizer:\\n{tokens}\")\n",
        "\n",
        "        encoded_input = tokenizer(sample_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        print(f\"\\nEncoded input (for model input):\\n{encoded_input}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not demonstrate tokenization. Error: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Download NLTK data if not already present (used for sentence tokenization if needed)\n",
        "    # nltk.download('punkt')\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9Xp7al6BLt4q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}